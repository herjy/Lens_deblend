{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages and setup\n",
    "import numpy as np\n",
    "import scarlet\n",
    "import MuSCADeT as wine\n",
    "from MuSCADeT  import colour_subtraction as cs\n",
    "import sep\n",
    "import scipy.signal as scp\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scarlet_extensions.initialization.detection import mad_wavelet, Data\n",
    "\n",
    "import astropy.io.fits as fits\n",
    "from astropy.wcs import WCS\n",
    "import csv\n",
    "\n",
    "# use a good colormap and don't interpolate the pixels\n",
    "matplotlib.rc('image', cmap='inferno', interpolation='none', origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "group = 'Maybe_lenses'\n",
    "\n",
    "if group == 'Group1':\n",
    "    cat = open(group+'/group1.csv')\n",
    "    filenames = [] \n",
    "    ids = []\n",
    "    for row in cat:\n",
    "        filenames.append(row.split(',')[1])\n",
    "        ids.append(filenames[-1][4:12])\n",
    "elif group == 'Group2':\n",
    "    cat = open(group+'/final_SL_b80-90.csv')\n",
    "    filenames = [] \n",
    "    ids = []\n",
    "    for row in cat:\n",
    "        filenames.append(row.split(',')[19])\n",
    "        ids.append(filenames[-1][4:12])\n",
    "elif group == 'Maybe_lenses':\n",
    "    cat = open(group+'/ML_final.csv')\n",
    "    filenames = [] \n",
    "    ids = []\n",
    "    for i, row in enumerate(cat):\n",
    "        filenames.append(row.split(',')[21])\n",
    "        ids.append(filenames[-1][4:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_psf = scarlet.GaussianPSF(sigma = [[.5, 0.5]])\n",
    "filters = ['g','r','i']\n",
    "\n",
    "#Scarlet plots\n",
    "norm = scarlet.display.AsinhMapping(minimum=-1, stretch=50, Q=10)\n",
    "norm_psf = scarlet.display.AsinhMapping(minimum=0, stretch=0.01, Q=10)\n",
    "\n",
    "def makeCatalog(datas, thresh = 3, lvl=3, wave=True):\n",
    "    ''' Creates a detection catalog by combining low and high resolution data\n",
    "\n",
    "    This function is used for detection before running scarlet.\n",
    "    It is particularly useful for stellar crowded fields and for detecting high frequency features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datas: array\n",
    "        array of Data objects\n",
    "    lvl: int\n",
    "        detection lvl\n",
    "    wave: Bool\n",
    "        set to True to use wavelet decomposition of images before combination\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    catalog: sextractor catalog\n",
    "        catalog of detected sources\n",
    "    bg_rms: array\n",
    "        background level for each data set\n",
    "    '''\n",
    "    if len(datas) == 1:\n",
    "        hr_images = datas[0].images / np.sum(datas[0].images, axis=(1, 2))[:, None, None]\n",
    "        # Detection image as the sum over all images\n",
    "        detect_image = np.sum(hr_images, axis=0)\n",
    "    else:\n",
    "        data_lr, data_hr = datas\n",
    "        # Create observations for each image\n",
    "        # Interpolate low resolution to high resolution\n",
    "        interp = interpolate(data_lr, data_hr)\n",
    "        # Normalisation of the interpolate low res images\n",
    "        interp = interp / np.sum(interp, axis=(1, 2))[:, None, None]\n",
    "        # Normalisation of the high res data\n",
    "        hr_images = data_hr.images / np.sum(data_hr.images, axis=(1, 2))[:, None, None]\n",
    "        # Detection image as the sum over all images\n",
    "        detect_image = np.sum(interp, axis=0) + np.sum(hr_images, axis=0)\n",
    "        detect_image *= np.sum(data_hr.images)\n",
    "    if np.size(detect_image.shape) == 3:\n",
    "        if wave:\n",
    "            # Wavelet detection in the first three levels\n",
    "            wave_detect = scarlet.Starlet.from_image(detect_image.mean(axis=0), scales=lvl+1).coefficients\n",
    "            wave_detect[:, -1, :, :] = 0\n",
    "            detect = scarlet.Starlet(coefficients=wave_detect).image\n",
    "        else:\n",
    "            # Direct detection\n",
    "            detect = detect_image.mean(axis=0)\n",
    "    else:\n",
    "        if wave:\n",
    "            wave_detect = scarlet.Starlet.from_image(detect_image, scales=lvl+1).coefficients\n",
    "            detect = np.mean(wave_detect[:-1], axis=0)\n",
    "        else:\n",
    "            detect = detect_image\n",
    "\n",
    "    bkg = sep.Background(detect)\n",
    "    catalog = sep.extract(detect, thresh, err=bkg.globalrms)\n",
    "\n",
    "    if len(datas) ==1:\n",
    "        bg_rms = mad_wavelet(datas[0].images)\n",
    "    else:\n",
    "        bg_rms = []\n",
    "        for data in datas:\n",
    "            bg_rms.append(mad_wavelet(data.images))\n",
    "\n",
    "    return catalog, bg_rms\n",
    "\n",
    "\n",
    "\n",
    "def make_obs(images, psf, wcs):\n",
    "    \n",
    "    data =  Data(images, wcs, psf, filters)\n",
    "    catalog, bg_rms = makeCatalog([data], lvl =0, thresh = 1, wave=True)\n",
    "\n",
    "    weights = np.ones_like(images) / (bg_rms**2)[:, None, None]\n",
    "    \n",
    "    model_frame = scarlet.Frame(\n",
    "        images.shape,\n",
    "        psf=model_psf,\n",
    "        channels=filters)\n",
    "\n",
    "    observation = scarlet.Observation(\n",
    "        images, \n",
    "        psf=scarlet.ImagePSF(psf), \n",
    "        weights=weights, \n",
    "        channels=filters).match(model_frame)\n",
    "    return model_frame, observation, catalog\n",
    "\n",
    "def make_sources(observation, model_frame, catalog):\n",
    "    starlet_sources = []\n",
    "    \n",
    "    n,n1,n2 = observation.data.shape\n",
    "    pixels = np.stack((catalog['y'], catalog['x']), axis=1)\n",
    "    if np.size(pixels)==0:\n",
    "        pixels=np.array([[n1/2., n2/2.]])\n",
    "        \n",
    "    r = np.sqrt(np.sum((pixels-np.array([n1/2., n2/2.]))**2, axis = 1))\n",
    "    lens = pixels[r == np.min(r)]\n",
    "    \n",
    "    sources = []\n",
    "    if np.size(catalog['y']) == 0:\n",
    "        new_source = scarlet.ExtendedSource(model_frame, \n",
    "                                            (n1/2., n2/2.), \n",
    "                                            observation, \n",
    "                                            K=1,\n",
    "                                            compact = 1)\n",
    "        sources.append(new_source)\n",
    "    \n",
    "    for k,src in enumerate(catalog):\n",
    "        \n",
    "        new_source = scarlet.ExtendedSource(model_frame, \n",
    "                                            (src['y'], src['x']), \n",
    "                                            observation, \n",
    "                                            K=1,\n",
    "                                            compact = 1)\n",
    "        sources.append(new_source)\n",
    "    \n",
    "    new_source = scarlet.StarletSource(model_frame, \n",
    "                                       (n1/2., n2/2.), \n",
    "                                       [observation], \n",
    "                                       spectrum = np.array([1.,1.,0.5]),\n",
    "                                       starlet_thresh = 0.1)\n",
    "    sources.append(new_source)\n",
    "    print(np.size(sources))\n",
    "    blend = scarlet.Blend(sources, observation)\n",
    "    return sources, blend\n",
    "\n",
    "\n",
    "def run_scarlet(blend, sources):\n",
    "    blend.fit(200, e_rel = 1.e-6) #Set iterations to 200 for better results\n",
    "    print(\"scarlet ran for {0} iterations to logL = {1}\".format(len(blend.loss), -blend.loss[-1]))\n",
    "    plt.plot(-np.array(blend.loss))\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('log-Likelihood')\n",
    "    \n",
    "    scarlet.display.show_scene(sources, \n",
    "                           norm=norm, \n",
    "                           observation=observation, \n",
    "                           show_rendered=True, \n",
    "                           show_observed=True, \n",
    "                           show_residual=True,\n",
    "                          )\n",
    "    scarlet.display.show_sources(sources,  \n",
    "                             norm = norm,\n",
    "                             observation=observation,\n",
    "                             show_rendered=True, \n",
    "                             show_observed=True,\n",
    "                             add_boxes=True\n",
    "                            )\n",
    "    plt.show()\n",
    "    \n",
    "def run_MuSCADeT(images, psf,A):\n",
    "    images = images[::-1]\n",
    "    psf = psf[::-1]\n",
    "    S, An = wine.MCA.mMCA(images, A.T, 5, 200, mode = 'None', PSF=psf, plot = True, PCA= [2,50])\n",
    "    n, n1,n2 = np.shape(images)\n",
    "    A=An\n",
    "    \n",
    "    # Models as extracted by MuSCADeT for display\n",
    "    model = np.dot(A,S.reshape([A.shape[1], n1*n2])).reshape(images.shape)\n",
    "    for i in range(n):\n",
    "        model[i] = scp.fftconvolve(model[i], psf[i], mode = 'same')\n",
    "    \n",
    "    normodel = cs.asinh_norm(model, Q=20, range = 50)\n",
    "    normcube = cs.asinh_norm((images), Q = 20, range = 50)\n",
    "    normres = cs.asinh_norm(images-model, Q = 10, range = 50)\n",
    "    plt.figure(figsize = (15, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('model')\n",
    "    plt.imshow(normodel)\n",
    "    plt.subplot(132)\n",
    "    plt.title('data')\n",
    "    plt.imshow(normcube)\n",
    "    plt.subplot(133)\n",
    "    plt.title('Residuals')\n",
    "    plt.imshow(normres)\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        C = A[:,i, np.newaxis, np.newaxis]*S[np.newaxis,i,:,:]\n",
    "        for j in range(n):\n",
    "            C[j] = scp.fftconvolve(C[j], psf[j], mode = 'same')\n",
    "        normC = cs.asinh_norm(C, Q = 20, range = 50)\n",
    "        normCres = cs.asinh_norm((images-C), Q = 20, range = 50)\n",
    "        if i == 0:\n",
    "            red = images-C\n",
    "            red_model = C\n",
    "        else:\n",
    "            blue = images-C\n",
    "            blue_model = C\n",
    "        plt.figure(figsize = (15, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('data')\n",
    "        plt.imshow(normcube)\n",
    "        plt.subplot(132)\n",
    "        plt.title('component ' + str(i))\n",
    "        plt.imshow(normC)\n",
    "        plt.subplot(133)\n",
    "        plt.title('data - component ' + str(i))\n",
    "        plt.imshow(normCres)\n",
    "        plt.show()\n",
    "        \n",
    "    image = images\n",
    "    residuals = images-model\n",
    "    return image, red, blue, red_model, blue_model, residuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "images_tab = []\n",
    "reds = []\n",
    "blues = []\n",
    "red_models = []\n",
    "blue_models = []\n",
    "residuals = []\n",
    "files = []\n",
    "for i,f in enumerate(filenames[1:]):\n",
    "    try:\n",
    "        print(f)\n",
    "        hdu = fits.open(group+'/data/'+f)\n",
    "    except:\n",
    "        print('failed file:', f)\n",
    "        continue\n",
    "    images = hdu[0].data\n",
    "    wcs = WCS(hdu[0].header)\n",
    "    psf_tab = []\n",
    "    images = []\n",
    "    size = []\n",
    "    for j,n in enumerate(['g','r','i']):\n",
    "        images.append(hdu[j].data)\n",
    "        psf_hdu = fits.open(group+'/psf/'+n+'/PSF_'+ids[i+1]+'.fits')\n",
    "        p = []\n",
    "        c = 0\n",
    "        while 1:\n",
    "            try: \n",
    "                p.append(psf_hdu[c].data) \n",
    "            except:\n",
    "                break\n",
    "            c+=1\n",
    "        size.append(p[-1].shape[0])\n",
    "        psf_tab.append(p)\n",
    "    npsf = np.max(size)\n",
    "    n1,n2 = np.shape(hdu[j].data)\n",
    "   \n",
    "    psf = np.zeros((3, npsf, npsf))\n",
    "\n",
    "    for j, p in enumerate(psf_tab):\n",
    "        psf[j, \n",
    "            np.floor((npsf-size[j])/2.).astype(int):npsf-np.floor((npsf-size[j])/2.).astype(int),\n",
    "            np.floor((npsf-size[j])/2.).astype(int):npsf-np.floor((npsf-size[j])/2.).astype(int)] =  p\n",
    "    psf = np.array(psf)\n",
    "    images = np.array(images)\n",
    "    images_rgb = scarlet.display.img_to_rgb(images, norm=norm)\n",
    "    psf_rgb = scarlet.display.img_to_rgb(psf, norm=norm_psf)\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(images_rgb)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(psf_rgb)\n",
    "    plt.show()\n",
    "    \n",
    "    frame, observation, cat = make_obs(images, psf, wcs)\n",
    "    sources, blend = make_sources(observation, frame, cat)\n",
    "    run_scarlet(blend, sources)\n",
    "    \n",
    "    psf = observation.renderer.diff_kernel._image\n",
    "    A = []\n",
    "    \n",
    "    bluer = np.array([0.667,0.333,0])\n",
    "    redder = np.array([0,0.333,0.667])\n",
    "    spectrum = []\n",
    "    rs = []\n",
    "    for i,s in enumerate(sources):\n",
    "        spec = s.get_model().sum(axis=(1, 2))\n",
    "        spectrum.append(spec/np.sum(spec))\n",
    "        origin = [s.bbox.origin[-2]+s.bbox.shape[-2]/2, s.bbox.origin[-1]+s.bbox.shape[-1]/2]\n",
    "        rs.append(np.sqrt((n1/2.-origin[-2])**2+(n2/2.-origin[-1])**2))\n",
    "    \n",
    "    blue = np.sum(bluer * spectrum, axis = 1)\n",
    "    red = np.sum(redder * spectrum, axis = 1)\n",
    "    rs = rs[:-1]\n",
    "    #Blue spectra\n",
    "    \n",
    "    \n",
    "    if np.argmax(blue) == np.argmin(rs):\n",
    "        rs[np.argmin(rs)] += n1/2\n",
    "    print(rs, np.argmin(rs))\n",
    "    #Red spectra\n",
    "    A.append(spectrum[np.argmax(red)])#np.argmin(rs)])\n",
    "    A.append(spectrum[np.argmax(blue)])\n",
    "\n",
    "    image, red, blue, red_model, blue_model, res = run_MuSCADeT(images, psf, np.array(A)[:,::-1])\n",
    "    images_tab.append(image)\n",
    "    reds.append(red)\n",
    "    blues.append(blue)\n",
    "    red_models.append(red_model)\n",
    "    blue_models.append(blue_model)\n",
    "    residuals.append(res)\n",
    "    files.append(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"MuSCADeT_models_\"+group+\".pkl\",\"wb\")\n",
    "pickle.dump([files, images, blues, reds, blue_models, red_models, residuals], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pickle.load(open(\"MuSCADeT_models_\"+group+\".pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.size(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
